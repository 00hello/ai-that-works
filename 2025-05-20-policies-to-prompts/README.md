---
"guid": "aitw-006"
"title": "S02E02 â€“ Policy to Prompt: Evaluating w/ the Enron Emails Dataset"
"description": "One of the most common problems in AI engineering is looking at a set of policies/rules and evaluating evidence to determine if the rules were followed. In this session we'll explore turning policies into prompts and pipelines to evaluate which emails in the massive Enron email dataset violated SEC and Sarbanes-Oxley regulations."
"event_link": "https://lu.ma/iw1d9l3j"
"eventDate": "2025-05-20T18:00:00Z"
"media":
  "url": "https://www.youtube.com/watch?v=gkekVC67iVs"
  "type": "video/youtube"
"links":
  "youtube": "https://www.youtube.com/watch?v=gkekVC67iVs"
  "rsvp": "https://lu.ma/iw1d9l3j"
  "code": "https://github.com/BoundaryML/ai-that-works/tree/main/2025-05-20-policies-to-prompts"
"season": 2
"episode": 2
"event_type": "episode"
---

# ðŸ¦„ policy to prompt: evaluating the enron email dataset against SEC regulations

one of the most common problems in AI engineering is looking at a set of policies / rules and evaluating evidence to determine if the rules were followed. In this session we'll explore turning policies into prompts and pipelines to evaluate which emails in the massive [enron email dataset](https://www.cs.cmu.edu/~enron/) violated SEC and Sarbanes-Oxley regulations.

[Video](https://www.youtube.com/watch?v=gkekVC67iVs) â€¢ [RSVP](https://lu.ma/iw1d9l3j)

<a href="https://www.youtube.com/watch?v=gkekVC67iVs"><img width="1019" alt="Screenshot 2025-05-22 at 10 29 53â€¯PM" src="https://github.com/user-attachments/assets/68c43941-f249-4c92-9a69-54db5e4a62ee" /></a>


## Key Topics

1. Policy-to-Prompt Workflows
    - Mapping compliance policies (Sarbanes-Oxley, JP Morgan Code of Conduct) to automated LLM checks
    - Focusing on specific rules (gift-giving) rather than generic policy systems
    - Building targeted evaluation pipelines

1. Iterative Evaluation Loop
    - Start with vibe evals (playground testing)
    - Add deterministic pytest cases
    - Capture intermediate pipeline steps
    - Use structured outputs (e.g. Pydantic models)

3. Scaling & Tooling Patterns
    - Regex pre-filtering â†’ async LLM calls â†’ structured analysis
    - Parallel processing with asyncio.gather
    - Batch processing for large datasets
    - Progress tracking with tqdm

4. Human-in-the-Loop & Golden Dataset
    - Store analyzed emails as JSON files
    - Enable reviewer triage of high-risk cases
    - Build golden dataset from production traffic
    - Monitor for drift and expand test cases

Aside - 12-Factor / ShadCN-for-Agents Mindset
- Open, customizable scaffold approach vs closed systems
- Developers own and version their agent code
- Flexibility to tweak and adapt


## Whiteboards

![image](https://github.com/user-attachments/assets/fcd7f73b-ee1f-485d-8771-f09176b54196)

![image](https://github.com/user-attachments/assets/d18c4c82-e3b2-4eca-922a-b5e80f37956f)

![image](https://github.com/user-attachments/assets/ddd2cddc-a596-4ef0-8543-4aacbbd76a7f)

![image](https://github.com/user-attachments/assets/c76ab794-5f21-4e07-963e-2f65c6b7cbf5)


## Running this code

### installing dependencies

```bash
# Install dependencies
uv sync
```

### Download the datasetsa

```bash
uv run datasets.py

```



### Run the code

```
# Run the code:
python pipeline.py
```
