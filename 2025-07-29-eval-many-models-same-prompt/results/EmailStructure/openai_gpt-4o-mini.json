{"subject": "Recap: Cracking the Prompting Interview", "body": "Hello [First Name],\n\nThis week's \ud83e\udd84 ai that works session was all about \"Cracking the Prompting Interview\"!\n\nThe full recording, whiteboard diagrams, and code from the session are now available on GitHub:\nhttps://github.com/hellovai/ai-that-works\n\nWe covered a lot on prompt engineering techniques and LLM optimization strategies. Here\u2019s a super quick recap:\n\n- **Shift Complexity to Code:** Don't ask an LLM to perform complex sorting or rigid formatting. Use it for the creative/reasoning parts and handle deterministic tasks in your application code for better reliability.\n- **Optimize for Tokens:** Instead of having the LLM repeat long pieces of text, have it output a simple index or alias. You can then map this back to the full text in your code, saving significant token costs.\n- **Guide with Inline Comments:** Use comments inside your prompt (e.g., `<!-- think step by step -->`) to steer the LLM's reasoning process without it appearing in the final output.\n- **Structure Over Examples:** A well-structured prompt with clear instructions and defined output formats (like JSON) is often more effective and token-efficient than providing multiple real-world examples.\n\nIf you remember one thing from this session:\n**RTFP (Read The Full Prompt)!** Before you debug your code or the model, always re-read your prompt carefully. The most common source of error is the LLM interpreting your instructions differently than you intended.\n\nOur next session on July 15th, 2025 will be all about \"Generating AI-powered Content with LLMs\" \u2013 exploring how to use AI pipelines to create content for various use cases.", "call_to_action": "Sign up here: https://lu.ma/ai-that-works-12"}