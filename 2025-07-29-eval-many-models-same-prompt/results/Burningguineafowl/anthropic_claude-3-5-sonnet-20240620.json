{
  "subject": "\ud83e\udd84 ai that works: Advanced Prompting & LLM Optimization Recap",
  "body": "Hello First Name,\n\nThis week's \ud83e\udd84 ai that works session was on \"Advanced Prompting & LLM Optimization\"!\n\nThe full recording, code, and diagrams from the session are now available on GitHub:\nhttps://github.com/hellovai/ai-that-works\n\nWe covered a lot on advanced prompt engineering and LLM optimization strategies. Here\u2019s a super quick recap:\n\nTreat Prompts as an API, Not a Magic Box: The most robust systems offload complexity from the prompt into deterministic code. Instead of asking an LLM to do complex formatting or follow convoluted business logic, use it for the core reasoning task. Then, use your own code to validate and structure the final output.\n\nOptimize for Tokens and Reasoning: Reduce costs and improve latency by designing prompts that return aliases or indexes instead of full-text sentences. To improve accuracy, use inline comments within the prompt to guide the LLM's \"chain of thought\" without adding unnecessary tokens to the final output.\n\nIf you remember one thing from this session:\nEffective prompt engineering is system design. It\u2019s about building a reliable pipeline by separating LLM reasoning from deterministic code and optimizing the data flow in and out of the model.\n\nOur next session on July 15th, 2025 will be all about \"Generating AI-Powered Content with LLMs\" \u2013 exploring how to use LLMs to generate content for various use cases.",
  "call_to_action": "Sign up here: https://lu.ma/ai-that-works-12"
}
